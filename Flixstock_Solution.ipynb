{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\n\nimport numpy as np\n\nfrom tqdm.auto import tqdm\n\nimport skimage\nfrom skimage import io\n\nimport matplotlib.pyplot as plt\n\nfrom annoy import AnnoyIndex # [1]\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\n\nimport albumentations as A # [2]\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:37.288942Z","iopub.execute_input":"2022-08-07T03:26:37.289264Z","iopub.status.idle":"2022-08-07T03:26:43.893064Z","shell.execute_reply.started":"2022-08-07T03:26:37.289184Z","shell.execute_reply":"2022-08-07T03:26:43.892255Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# CLASS DEFINITIONS","metadata":{}},{"cell_type":"code","source":"class Helper():\n    def __init__(self):\n        pass\n    def sample_plot(x, figsize = (25, 25), num = 16):\n        '''Plots the sample for the dataset\n        x: -> Data\n        figsize: -> Tuple, telling the size of the figure\n        num: -> Number of samples that are to be plotted'''\n        fig = plt.figure(figsize = figsize)\n        x = x.detach().permute(0, 2, 3, 1).cpu().numpy()\n        for i in range(num):\n            plt.subplot(5, 5, i + 1, xticks = [], yticks = [])\n            plt.imshow(x[i])\n    def read_img(addr):\n        '''Takes URI as an input and returns an image\n        addr: -> A URI string that is the address for the input image'''\n        img = io.imread(addr)\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:49.544907Z","iopub.execute_input":"2022-08-07T03:26:49.545648Z","iopub.status.idle":"2022-08-07T03:26:49.551819Z","shell.execute_reply.started":"2022-08-07T03:26:49.545616Z","shell.execute_reply":"2022-08-07T03:26:49.550876Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class get_data(Dataset):\n    '''Used along with dataloader to load the dataset'''\n    def __init__(self, files_list):\n        super(get_data, self).__init__()\n        self.files = files_list\n        self.tfms = A.Compose([\n            A.Resize(224, 224),\n            A.Normalize(mean = 0.0, std = 1.0, max_pixel_value = 255.0),\n            ToTensorV2()\n        ])\n    def __len__(self):\n        return len(self.files)\n    def __getitem__(self, idx):\n        x = self.tfms(image = io.imread(self.files[idx]))['image'].to(DEVICE)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:52.366677Z","iopub.execute_input":"2022-08-07T03:26:52.367579Z","iopub.status.idle":"2022-08-07T03:26:52.375884Z","shell.execute_reply.started":"2022-08-07T03:26:52.367533Z","shell.execute_reply":"2022-08-07T03:26:52.373378Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Network(nn.Module):\n    '''This class defines the architecture of the network'''\n    def __init__(self):\n        super(Network, self).__init__()\n        self.model = models.resnet50(pretrained = True)\n        weight = self.model.conv1.weight.clone()\n        self.model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False) # Altering the model to accomodate 4-channel input\n        with torch.no_grad():\n            self.model.conv1.weight[:, :3] = weight\n            self.model.conv1.weight[:, 3] = self.model.conv1.weight[:, 0]\n        self.model.avgpool = nn.Identity()\n        self.model.fc = nn.Identity()\n        for param in self.model.parameters():\n            param.requires_grad = False\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:53.389765Z","iopub.execute_input":"2022-08-07T03:26:53.390126Z","iopub.status.idle":"2022-08-07T03:26:53.398116Z","shell.execute_reply.started":"2022-08-07T03:26:53.390082Z","shell.execute_reply":"2022-08-07T03:26:53.397124Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class solution(nn.Module):\n    def __init__(self, inp, model, ann_idx, files_list, num):\n        super(solution, self).__init__()\n        self.num = num\n        self.files_list = files_list\n        tfms = A.Compose([\n            A.Resize(224, 224),\n            A.Normalize(mean = 0.0, std = 1.0, max_pixel_value = 255.0),\n            ToTensorV2()\n        ])\n        self.q = inp\n        emb = model(torch.unsqueeze(tfms(image = inp)['image'].cuda(), axis = 0)).detach().cpu().numpy().flatten()\n        ann_idx.add_item(len_db + 1, emb)\n        self.similar = ann_idx.get_nns_by_item(len_db + 1, self.num)\n    def plot_sim(self):\n        '''Plots similar images to the input'''\n        plt.figure(figsize = (10, 10))\n        plt.subplot()\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(self.q)\n        title = plt.title('Query Image')\n        plt.setp(title, color = 'green')\n        plt.figure(figsize = (25, 25))\n        for i in range(self.num - 1):\n            plt.subplot(5, 5, i + 1, xticks = [], yticks = [])\n            plt.imshow(io.imread(self.files_list[self.similar[i + 1]]))\n            title = plt.title('Similar Garment')\n            plt.setp(title, color = 'green')","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:54.328218Z","iopub.execute_input":"2022-08-07T03:26:54.328786Z","iopub.status.idle":"2022-08-07T03:26:54.339143Z","shell.execute_reply.started":"2022-08-07T03:26:54.328750Z","shell.execute_reply":"2022-08-07T03:26:54.338343Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# DEVICE SETUP","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    DEVICE = \"cuda:0\"\n    torch.cuda.empty_cache()\nelse:\n    DEVICE = \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:55.096193Z","iopub.execute_input":"2022-08-07T03:26:55.096902Z","iopub.status.idle":"2022-08-07T03:26:55.171882Z","shell.execute_reply.started":"2022-08-07T03:26:55.096864Z","shell.execute_reply":"2022-08-07T03:26:55.170658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# DATASET LOADING AND VISUALIZATION","metadata":{}},{"cell_type":"code","source":"files_list = glob.glob('../input/flixstock/bottoms_resized_png/*.png')\nlen_db = len(files_list)\nnum = 10\n\nDB = get_data(files_list)\ndb = DataLoader(DB, shuffle = False, batch_size = len(os.listdir('../input/flixstock/bottoms_resized_png')))\n\nx = next(iter(db))\n\nHelper.sample_plot(x)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:26:56.206800Z","iopub.execute_input":"2022-08-07T03:26:56.207569Z","iopub.status.idle":"2022-08-07T03:27:16.548463Z","shell.execute_reply.started":"2022-08-07T03:26:56.207534Z","shell.execute_reply":"2022-08-07T03:27:16.547455Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# MODEL DEFINITION AND FEATURE MAP GENERATION","metadata":{}},{"cell_type":"code","source":"model = Network().to(DEVICE)\n\nfeature_vec = []\n\nfor i in tqdm(range(len_db)):\n    with torch.no_grad():\n        fv = model(torch.unsqueeze(x[i], axis = 0)).detach().cpu().numpy().flatten()\n    feature_vec.append(fv)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:27:16.550420Z","iopub.execute_input":"2022-08-07T03:27:16.550675Z","iopub.status.idle":"2022-08-07T03:27:36.651801Z","shell.execute_reply.started":"2022-08-07T03:27:16.550643Z","shell.execute_reply":"2022-08-07T03:27:36.651111Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# INDEX CREATION","metadata":{}},{"cell_type":"code","source":"feature_len = len(feature_vec[0])\nann_idx = AnnoyIndex(feature_len, 'angular')\n\nfor i in tqdm(range(len_db)):\n    ann_idx.add_item(i, feature_vec[i])\n\nann_idx.build(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T03:27:38.624171Z","iopub.execute_input":"2022-08-07T03:27:38.624511Z","iopub.status.idle":"2022-08-07T03:27:53.972997Z","shell.execute_reply.started":"2022-08-07T03:27:38.624476Z","shell.execute_reply":"2022-08-07T03:27:53.972321Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# OUTPUT","metadata":{}},{"cell_type":"code","source":"q_img = Helper.read_img(addr = files_list[150])\nsol = solution(q_img, model, ann_idx, files_list, num + 1)\nsol.plot_sim()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T15:45:07.186431Z","iopub.execute_input":"2022-08-06T15:45:07.186726Z","iopub.status.idle":"2022-08-06T15:45:08.279007Z","shell.execute_reply.started":"2022-08-06T15:45:07.186696Z","shell.execute_reply":"2022-08-06T15:45:08.278213Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# REFERENCES\n\n[1] Erik Bernhardsson. (2018). Annoy: Approximate Nearest Neighbors in C++/Python.\n\n[2] A. Buslaev, V., & A.~A. Kalinin (2018). Albumentations: fast and flexible image augmentations. ArXiv e-prints.","metadata":{}}]}